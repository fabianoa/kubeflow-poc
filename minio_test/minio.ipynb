{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "from typing import NamedTuple\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In v1.1.0, in-cluster communication from notebook to Kubeflow Pipeline is not supported in this phase.\n",
    "# In order to use kfp as previous, user needs to pass a cookie to KFP for communication as a workaround.\n",
    "# https://www.kubeflow.org/docs/aws/pipeline/#authenticate-kubeflow-pipeline-using-sdk-inside-cluster\n",
    "\n",
    "#authservice_session='authservice_session=<cookie>'\n",
    "client = kfp.Client()\n",
    "#Mudar namespace\n",
    "namespace='fabiano-alencar'\n",
    "client.list_experiments(namespace=namespace)\n",
    "DATA_PATH = '/mnt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component: Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(data_path,output_data_path: OutputPath()):\n",
    "    \n",
    "    \n",
    "    #from minio import Minio\n",
    "    #from minio.error import S3Error\n",
    "\n",
    "    #client = Minio(\n",
    "    #\"minio-service.kubeflow:9000\",\n",
    "    #access_key=\"minio\",\n",
    "    #secret_key=\"minio123\",\n",
    "    #secure=False,\n",
    "    #)\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    #buckets = client.list_buckets()\n",
    "    #print(buckets)\n",
    "    import subprocess\n",
    "    \n",
    "    subprocess.call(['mkdir', '/mnt/data'])\n",
    "    \n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "    \n",
    "    s3 = boto3.resource('s3',\n",
    "                    endpoint_url='http://minio-service.kubeflow:9000',\n",
    "                    config=Config(signature_version='s3v4'))\n",
    "    \n",
    "    print(data_path)\n",
    "    #s3.Bucket('ialab').download_file('dados/labelled/frente-verso/cnh-rg-oab-cpf/frente/relatorio_tarefa_457590379_p7_2.png',output_data_path)\n",
    "    \n",
    "    bucket = s3.Bucket('ialab')\n",
    "    folder_prefix = 'dados/labelled/frente-verso/cnh-rg-oab-cpf/frente'\n",
    "\n",
    "    for s3_file in bucket.objects.filter(Prefix=folder_prefix):\n",
    "        file_object = s3_file.key\n",
    "        file_name = str(file_object.split('/')[-1])\n",
    "        print('Downloading file {} ...'.format(file_object))\n",
    "        # outputpath\n",
    "        bucket.download_file(file_object, output_data_path)\n",
    "        #volume \n",
    "        bucket.download_file(file_object, data_path+\"/data/\"+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_minio_op = comp.func_to_container_op(load_raw_data,\n",
    "                                             base_image='python:3.7-slim',\n",
    "                                             packages_to_install=['boto3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_path):\n",
    "    \n",
    "    import subprocess\n",
    "\n",
    "    # downlaod the dataset from the mlflow repo\n",
    "    def install():\n",
    "        subprocess.call(['apt-get', 'update'])\n",
    "        subprocess.call(['apt-get', 'install', 'ffmpeg', '-y'])\n",
    "        subprocess.call(['apt-get', 'install', 'libxext6', '-y'])\n",
    "        subprocess.call(['apt-get', 'install', 'libsm6', '-y'])\n",
    "        subprocess.call(['apt-get', 'install', 'libfontconfig1', '-y'])\n",
    "        subprocess.call(['apt-get', 'install', 'libxrender1', '-y'])\n",
    "        subprocess.call(['apt-get', 'install', 'libgl1-mesa-glx', '-y'])\n",
    "     \n",
    "    install()\n",
    "    \n",
    "    import cv2\n",
    "    import glob\n",
    "    \n",
    "    ext = ['png', 'jpg', 'gif']    # Add image formats here\n",
    "\n",
    "    files = []\n",
    "    [files.extend(glob.glob(data_path+\"/data/\" + '*.' + e)) for e in ext]\n",
    "    print(files)\n",
    "    images = [cv2.imread(file) for file in files]\n",
    "    #print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_opencv_op = comp.func_to_container_op(process_data,\n",
    "                                             base_image='python:3.7-slim',\n",
    "                                             packages_to_install=['opencv-python','opencv-contrib-python','glob2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client.models import V1EnvVar\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Training pipeline',\n",
    "    description='Training pipeline for time series forecasting on household power consumption dataset.'\n",
    "\n",
    ")\n",
    "def training_pipeline(data_path):\n",
    "    \n",
    "    http_proxy = V1EnvVar(name='http_proxy', value='http://10.190.88.59:8888')\n",
    "    https_proxy = V1EnvVar(name='https_proxy', value='http://10.190.88.59:8888')\n",
    "    no_proxy = V1EnvVar(name='no_proxy', value='mlflow.mlflow,minio-service.kubeflow')   \n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    #vop = dsl.VolumeOp(\n",
    "    #name=\"create_volume\",\n",
    "    #resource_name=\"data-volume\", \n",
    "    #size=\"1Gi\", \n",
    "    #modes=dsl.VOLUME_MODE_RWM)\n",
    "    \n",
    "    kubeflow_pvc = dsl.PipelineVolume(pvc=\"workspace-server1\", name=\"workspace-server1\")\n",
    "    \n",
    "    \n",
    "    # Returns a dsl.ContainerOp class instance.\n",
    "    load_raw_data_task = test_minio_op(data_path).set_display_name('Load Raw Data') \\\n",
    "                .add_env_variable(http_proxy) \\\n",
    "                .add_env_variable(https_proxy) \\\n",
    "                .add_env_variable(no_proxy) \\\n",
    "                .apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n",
    "                .add_pvolumes({data_path: kubeflow_pvc}) \\\n",
    "    \n",
    "    \n",
    "    process_opencv_task = process_opencv_op(data_path).set_display_name('Process Images') \\\n",
    "                .add_env_variable(http_proxy) \\\n",
    "                .add_env_variable(https_proxy) \\\n",
    "                .add_env_variable(no_proxy) \\\n",
    "                .add_pvolumes({data_path: load_raw_data_task.pvolume})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/e71f471f-839a-4521-be8f-4c90ea134b65\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/d84456f3-d948-4a5e-bc67-5b528f851073\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=d84456f3-d948-4a5e-bc67-5b528f851073)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "experiment_name = 'minio_test'\n",
    "\n",
    "# Submit a pipeline run\n",
    "client.create_run_from_pipeline_func(\n",
    "    training_pipeline, arguments=arguments, namespace=namespace,experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the Pipeline to be reuseable by others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(training_pipeline, 'workflow.yaml')\n",
    "\n",
    "\n",
    "#client.upload_pipeline(pipeline_package_path='workflow.yaml',\n",
    "#                             pipeline_name='Electric Power Consumption Forecasting Training Pipeline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "docker_image": "gcr.io/arrikto-public/tensorflow-1.15.2-notebook-cpu:1.0.0.arr1",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "pipeline_description": "",
   "pipeline_name": "",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-server-k451mcy01",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
